{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45890.69748766, 49731.54139789])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy import stats\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "dr = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "housepath = os.path.join(\"datasets\", \"housing\")\n",
    "# print(housepath)\n",
    "houseurl = dr + \"datasets/housing/housing.tgz\"\n",
    "# print(houseurl)\n",
    "\n",
    "def fetch_housing_data(housing_url=houseurl, housing_path=housepath):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "#     print(tgz_path)\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_house_data(hp=housepath):\n",
    "    csv_path = os.path.join(hp, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def split_train_test(data, ratio):\n",
    "    shuffled = np.random.permutation(len(data))\n",
    "    testsize = int(len(data) * ratio)\n",
    "    test_indices = shuffled[:testsize]\n",
    "    train_indices = shuffled[testsize:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "housing = load_house_data()\n",
    "# housing.head()\n",
    "# housing.info()\n",
    "# housing['ocean_proximity'].value_counts()\n",
    "# housing.describe()\n",
    "# housing.hist(bins=50, figsize=(20,15))\n",
    "# plt.show()\n",
    "# traindf, testdf = split_train_test(housing, .2)\n",
    "# print(len(traindf), len(testdf))\n",
    "traindf, testdf = train_test_split(housing, test_size=.2, random_state=42)\n",
    "# stratified sampling using pandas\n",
    "# housing.info()\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], \n",
    "                              bins=[0.,1.5,3.0,4.5,6., np.inf],\n",
    "                              labels=[1,2,3,4,5])\n",
    "# rather than do it on our own, create the stratified data by calling scikit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=.2, random_state=42)\n",
    "for train_ind, test_ind in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_ind]\n",
    "    strat_test_set = housing.loc[test_ind]\n",
    "\n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "housing = strat_train_set.copy()\n",
    "# exploring data and plotting different types of scatter plots\n",
    "# housing.plot(kind=\"scatter\", x = \"longitude\", y = \"latitude\")\n",
    "# alpha parameter shows the density\n",
    "# housing.plot(kind=\"scatter\", x = \"longitude\", y = \"latitude\", alpha=.1)\n",
    "# housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = .4, \n",
    "#              s = housing[\"population\"]/100, label = \"population\", figsize = (10,7), \n",
    "#             c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar = True,)\n",
    "# plt.legend()\n",
    "# create a correlation matrix and display\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending = False)\n",
    "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
    "# create a scatter plot matrix and display\n",
    "# scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "# housing.plot(kind = 'scatter', x = 'median_income', y = 'median_house_value', alpha = .1)\n",
    "housing['rooms_per_household'] = housing['total_rooms'] / housing['households']\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms'] / housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population'] / housing['households']\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending = False)\n",
    "\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "housing_num = housing.drop('ocean_proximity', axis = 1)\n",
    "imputer.fit(housing_num)\n",
    "# imputer.statistics_\n",
    "# housing_num.median().values\n",
    "x = imputer.transform(housing_num)\n",
    "htr = pd.DataFrame(x, columns = housing_num.columns, index = housing_num.index)\n",
    "# htr.head()\n",
    "# print(type(htr))\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.head()\n",
    "# scikit is always using objects, for instance\n",
    "# we will use the ordinalencoder to discretize our\n",
    "# categorical variables\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "# discretizing returns numpy array\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]\n",
    "ordinal_encoder.categories_\n",
    "# use one hot encoding instead because ml will determine\n",
    "# that categories with shorter distance are similar\n",
    "# but this is not the case with our dataset.\n",
    "# one hot encoding gives a binary answer to if\n",
    "# the data belongs to certain category, inland, island, etc\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "# outputs a scipy sparse matrix instead of numpy array\n",
    "housing_cat_1hot\n",
    "# sparse matrix only stores locations of 1 in sea of 0s.\n",
    "# can convert to dense numpy array as follows\n",
    "housing_cat_1hot.toarray()\n",
    "cat_encoder.categories_\n",
    "# if there are many categories, one hot encoding will cause\n",
    "# large number of input features, can slow down performance\n",
    "# could just replace categorical input with relevant feature,\n",
    "# instead of this category we could put the actual distance to the ocean\n",
    "# or you could replace each category iwth low dimension vector called an embedding\n",
    "# each category's representation will be learned in training. It is representation learning\n",
    "# chapters 13 and 17\n",
    "\n",
    "#create new class to transform data, basically custom transformation\n",
    "# attr_adder = CombinedAttributesAdder(add_bedrooms_per_room = False)\n",
    "# housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3,4,5,6\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        # no args or kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room = False)\n",
    "housing_etra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "#instead of doing the above, we can just do pipeline of processes\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('attribs_adder', CombinedAttributesAdder()),\n",
    "                        ('std_scaler', StandardScaler()),\n",
    "                        ])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "# do both in one instead\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "# this pipeline stuff is cool it reminds me of\n",
    "# shell project \n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "\n",
    "# well that was easy\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "sd = housing.iloc[:5]\n",
    "sl = housing_labels.iloc[:5]\n",
    "sdp = full_pipeline.transform(sd)\n",
    "# print('Predictions', lin_reg.predict(sdp))\n",
    "# print('Labels', list(sl))\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "# the linear regression model sucks for this\n",
    "# error of $68000 dollars\n",
    "lin_rmse\n",
    "\n",
    "# lets do a regression decision tree\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "\n",
    "def displayScores(scores):\n",
    "    print('Scores: ', scores)\n",
    "    print('Mean: ', scores.mean())\n",
    "    print('Standard deviation: ', scores.std())\n",
    "\n",
    "# let's use cross validation cause overfitting\n",
    "# scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv = 10)\n",
    "# tree_rmse_scores = np.sqrt(-scores)\n",
    "# displayScores(tree_rmse_scores)\n",
    "# lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv = 10)\n",
    "# lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "# displayScores(lin_rmse_scores)\n",
    "# decision tree is overfitting like crazy\n",
    "# how about we use the ensemble method of random forests\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(housing_prepared, housing_labels)\n",
    "# print('hi')\n",
    "# forscore = cross_val_score(forest, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv = 10)\n",
    "# forestrmse = np.sqrt(-forscore)\n",
    "# displayScores(forestrmse)\n",
    "# print('hello')\n",
    "\n",
    "# use grid search to tune hyperparameters\n",
    "param_grid = [\n",
    "    {'n_estimators': [3,10,30], 'max_features': [2,4,6,8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}\n",
    "]\n",
    "grid_search = GridSearchCV(forest, param_grid, cv = 5, scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_estimator_)\n",
    "# cvres = grid_search.cv_results_\n",
    "# for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "#     print(np.sqrt(-mean_score), params)\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_encoder = full_pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse = True)\n",
    "\n",
    "# now evaluate on the test set\n",
    "finalModel = grid_search.best_estimator_\n",
    "\n",
    "# x is data, y are labels\n",
    "XTest = strat_test_set.drop('median_house_value', axis = 1)\n",
    "yTest = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "XTestPrepared = full_pipeline.transform(XTest)\n",
    "\n",
    "finalPreds = finalModel.predict(XTestPrepared)\n",
    "finalMSE = mean_squared_error(yTest, finalPreds)\n",
    "finalRMSE = np.sqrt(finalMSE)\n",
    "\n",
    "confidence = .95\n",
    "squaredErrors = (finalPreds - yTest) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squaredErrors) - 1,\n",
    "                        loc = squaredErrors.mean(),\n",
    "                        scale = stats.sem(squaredErrors)))\n",
    "\n",
    "# lastly, introduced the idea of joblib to load and use the same model\n",
    "# again and again. Useful if you wanted to deploy on a server or make into\n",
    "# an api so you don't train a model every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
