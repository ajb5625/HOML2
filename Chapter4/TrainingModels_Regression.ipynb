{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "# y hat = theta0 + theta1x1 + ... + thetanxn\n",
    "# vectorized form : y hat = parameter vector * feature vector\n",
    "\n",
    "# to find theta vector that minimizes the cost (MSE)\n",
    "# there is a closed form solution = The Normal Equation\n",
    "# theta hat = (XTX)^-1 XT y\n",
    "# theta hat is value that minimizes cost function\n",
    "# y is vector of target values\n",
    "# now we test this equation as follows\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "# generated linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19638367],\n",
       "       [2.75376528]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, compute theta hat using normal equation\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best\n",
    "\n",
    "# the function we used to generate the data was\n",
    "# y = 4 + 3x + gaussian noise\n",
    "# so we would've hoped for theta0 to be 4 and theta1 to be 3, but close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19638367],\n",
       "       [9.70391423]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now can make predictions using theta hat\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict\n",
    "\n",
    "# pretty neat, the underworkings of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets plot it\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0,2,0,15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.19638367]), array([[2.75376528]]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do simply with scikit\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "# same as what we calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19638367],\n",
       "       [9.70391423]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)\n",
    "# and same as prediction above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19638367],\n",
       "       [2.75376528]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the linear regression class is based on scipy.linalg.lstsq\n",
    "# meaning least squares, can call directly\n",
    "\n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond = 1e-6)\n",
    "theta_best_svd\n",
    "\n",
    "# same as two above\n",
    "# this function actually computes\n",
    "# theta hat = X^+y, where X^+ is the pseudoinverse of X\n",
    "# (Moore-Penrose inverse). can use np.linalg.pinv() to compute \n",
    "# pseudoinverse directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19638367],\n",
       "       [2.75376528]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)\n",
    "\n",
    "# the purpose of the pseudoinverse in linear algebra is to\n",
    "# compute the best fit (least squares, eg linear regression) \n",
    "# solution to a system of linear equations that lacks a solution\n",
    "\n",
    "# another use is to find the minimum euclidean norm solution\n",
    "# to a system of linear equations with multiple solutions\n",
    "\n",
    "# it is computed using standard matrix factorization technique\n",
    "# called Singular Value Decomposition (SVD) that decomposes\n",
    "# the training set X into matrix multiplication of three \n",
    "# matrices U, sigma, and V^T (np.linalg.svd())\n",
    "# the pseudoinverse is computed as X^+ = V sigma^+ U^T\n",
    "\n",
    "# to compute sigma^+, algo takes sigma and sets to zero all values\n",
    "# smaller than a tiny threshold. after, it replaces all nonzero values with \n",
    "# their inverse and transposes the resulting matrix\n",
    "\n",
    "# this computation is more efficient than the normal equation\n",
    "# also, the normal equation may not work if XTX is not invertible,\n",
    "# but the pseudoinverse is always defined\n",
    "\n",
    "# normal equation complexity is between O(n^2.4) and O(n^3)\n",
    "# while scikit svd is O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent is better suited method to train\n",
    "# linear regression model when there are a large\n",
    "# number of features or too many training instances to fit\n",
    "# in memory\n",
    "\n",
    "# move in direction of steepest descent by computing the gradient\n",
    "# at the current point. when gradient is 0, ou have found minimum\n",
    "\n",
    "# start parameter vector with random values and improve gradually taking one\n",
    "# step at a time, attempting to decrease the cost function until\n",
    "# algorithm converges at minimum\n",
    "\n",
    "# important hyperparameter is learning rate. if too small, it will take \n",
    "# too many steps. if too large, it could oscillate and could diverge\n",
    "\n",
    "# if cost function is not bowl shape, could converge to local min\n",
    "# rather than global optimal min\n",
    "\n",
    "# mse is convex function, meaning pick any two points across\n",
    "# the curve and line connecting them never crosses the curve\n",
    "# meaning no local minima, just global minimum\n",
    "# it also is continuous (can take a derivative) and never\n",
    "# changes slope abruptly\n",
    "\n",
    "# when using gradient descent, make sure all features have similar scale\n",
    "# ie use standard scaler\n",
    "\n",
    "# training is more difficult for this in higher dimensions; thankfully for linear regression\n",
    "# the solution is simply at the bottom of the bowl (cause convex function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19638367],\n",
       "       [2.75376528]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch gradient descent\n",
    "# takes partial derivative of cost function with respect to each\n",
    "\n",
    "# partial derivative of cost function = \n",
    "# 2/m * sum(parameter .dot features - labels)*feature\n",
    "\n",
    "# parameter thetaj\n",
    "# to determine, take partials and move in opposite direction of slope\n",
    "# times the learning rate to get step size\n",
    "# theta step = theta - rate * gradient(mse)\n",
    "\n",
    "eta = .1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta\n",
    "# and is exactly what every other method found\n",
    "# to find a good learning rate, can use grid search from chapter 2\n",
    "# to find iterations, set to large number and break when gradient\n",
    "# vector becomes tiny, smaller than tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.09112173],\n",
       "       [2.80638026]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stochastic gradient descent is good when the training\n",
    "# data is huge; but because of random nature, can\n",
    "# oscillate when close to the minimum\n",
    "# because of this, it probably won't find global min\n",
    "\n",
    "# it also looks at one piece of data at a time and computes gradient\n",
    "# rather than batch of whole training data as in bgd\n",
    "\n",
    "# this means the training step is much faster but more random(stochastic)\n",
    "\n",
    "# when cost function is irregular, sgd can actually help algorithm jump out\n",
    "# of local minima, so sgd has better chance of finding global minimum than bgd\n",
    "\n",
    "# randomness is good because can help us escape local minima, but bad because\n",
    "# algorithm will never settle at global minimum\n",
    "# can fix this by gradually reducing the learning rate with a learning schedule\n",
    "\n",
    "# implementation of stochastic gradient descent with simple learning schedule\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1) # random 2x1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index + 1]\n",
    "        yi = y[random_index:random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.13081318]), array([2.79338524]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also do sgdregressor from sklearn\n",
    "# params for below example as follows\n",
    "# max 1000 epochs, or until loss drops by less than .001 (tolerance)\n",
    "# learning rate starts at .1 and follows a default learning schedule\n",
    "# does no regularization (penalty = none)\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter = 1000, tol = 1e-3, penalty = None, eta0 = .1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "# will again find solution close to one returned by normal equation\n",
    "\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini batch gradient descent looks at subsets of training data\n",
    "# can take advantage of hardware optimization of matrix operations on gpus\n",
    "\n",
    "# progress in parameter space is less erratic than sgd, will walk closer around\n",
    "# the global min than sgd but may be harder to escape the local minima (when problems\n",
    "# suffer this issue, unlike linear regression :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, onto Polynomial Regression\n",
    "\n",
    "# can use linear model to fit nonlinear data\n",
    "# add powers of each feature as new feature\n",
    "# then train linear model on this extended set of features\n",
    "\n",
    "# generate nonlinear data based on quadratic equation\n",
    "\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = .5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "# made some cool vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.35131908])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so, a line will never fit this data.\n",
    "# we can use scikits polynomial features class to transform \n",
    "# our training data, adding the square of each feature in the training\n",
    "# set as a new feature\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.35131908,  5.52870143])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]\n",
    "\n",
    "# X_poly contains the original feature of X plus the square of this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.26037694]), array([[1.02040147, 0.43869951]]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we can fit a linear regression model to this extended training data\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "# compare this to intercept = 2, x^2 = .5, x = 1\n",
    "\n",
    "# when there are multiple features, polynomial regression\n",
    "# can find relationships between features, which linear regression\n",
    "# cannot. this is because PolynomialFeatures adds all combinations\n",
    "# of features up to the given degree\n",
    "\n",
    "# ex if degree = 3 and there are features a and b, polynomial adds\n",
    "# a^2, a^3, b^2, b^3 and ab, a^2b, ab^2\n",
    "# this can cause a combinatorial explosion, be careful# so, a line will never fit this data.\n",
    "# we can use scikits polynomial features class to transform \n",
    "# our training data, adding the square of each feature in the training\n",
    "# set as a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if you perform high degree poly regression,\n",
    "# will likely fit the training model better than with\n",
    "# plain linear regression\n",
    "\n",
    "# the problem we see here is the polynomial regression\n",
    "# will overfit training data, linear regression will\n",
    "# underfit training data\n",
    "\n",
    "# we looked at k fold cross validation before.\n",
    "# a good way to analyze for regression models\n",
    "# is looking at the learning curves. This plots \n",
    "# the model's performance on training and validation sets\n",
    "# as a function of the training set size\n",
    "\n",
    "# so we will train model several times on different sized subsets\n",
    "# of data of training set.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), 'r-+', linewidth = 2, label = 'train')\n",
    "    plt.plot(np.sqrt(val_errors), 'b-', linewidth = 3, label = 'val')\n",
    "    \n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "\n",
    "# so this model is underfitting, which makes sense cause\n",
    "# data was not linear to begin with. we see high validation error\n",
    "# when training few examples because it cannot generalize well off of only \n",
    "# a few training examples. When learning more examples, val error slowly goes down \n",
    "# until it hits a plateau \n",
    "\n",
    "# with the train error, it is easy to correctly classify the first few training examples\n",
    "# but after a while, we see that a line does not fit nonlinear data well. it also plateaus\n",
    "# and meets around the same place as the validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now learning curves of 10th degree polynomial model (expecting overfitting)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree = 10, include_bias = False)),\n",
    "    (\"lin_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "\n",
    "# as we can see, error on training data is much lower here\n",
    "# than in regular linear regression\n",
    "# there is also a gap between the curves, meaning model\n",
    "# performs much better on training data than validation data\n",
    "# hallmark of overfitting model\n",
    "# but, when training set gets larger curves get much closer\n",
    "\n",
    "# and as we can see, one way to fix an overfitting model is\n",
    "# to feed it more training data until validation error meets\n",
    "# training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias/variance tradeoff\n",
    "# error of model can be described as sum of 3 different types of errors\n",
    "\n",
    "# bias : generalization error is due to wrong assumptions. assuming data is linear\n",
    "# when it is actually quadratic. high bias model is most likely to underfit training data\n",
    "\n",
    "# variance : due to model's excessive sensitivity to small variations in training data\n",
    "# a model with many degrees of freedom (high degree polynomial model) is likely to have\n",
    "# high variance and thus overfit training data\n",
    "\n",
    "# irreducible error : noisiness of data. reduce by cleaning data, fix data sources\n",
    "# detect or remove outliers\n",
    "\n",
    "# increasing a model's complexity typically increases its variance and reduces its bias\n",
    "# reducing a models complexity increases its bias and reduces its variance\n",
    "\n",
    "# this is why it is a tradeoff :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Linear Models\n",
    "\n",
    "# the fewer degrees of freedom a model has, thre harder it will\n",
    "# be to overfit the data\n",
    "\n",
    "# regularize polynomial model by reducing number of polynomial degrees\n",
    "\n",
    "# for linear model, regularization is achieved by constraining weights of the model\n",
    "\n",
    "# Ridge Regression, Lasso Regression, and Elastic Net are 3 different ways to constrain the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.08065386]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "# for the mse, it adds a regularization term theta^2\n",
    "# this insures that the weights are as small as possible\n",
    "# if w is feature weights, theta is 1/2(magn(w)_2)^2, where magn(w)_2\n",
    "# is the l2 norm of the weight vector\n",
    "# for gradient descent, just add alpha * weight vector to mse gradient vector\n",
    "\n",
    "# l1 norm is sum of absolute values of vector\n",
    "# l2 norm is square root of sum of squares (magnitude)\n",
    "\n",
    "# closed form solution for Ridge Regression, A is identity matrix with 0 in top\n",
    "# left cell for bias term\n",
    "\n",
    "# theta hat = (X^TX + alpha*A)^-1 * X^T * y\n",
    "\n",
    "# now using scikit, will use closed form solution that\n",
    "# uses Cholesky matrix factorization technique\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha = 1, solver = 'cholesky')\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.04369398])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also with sgd\n",
    "\n",
    "sgd_reg = SGDRegressor(penalty = 'l2')\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "# specifying penalty in sgd tells scikit you want to add\n",
    "# l2 norm, 1/2 magnitude of weight vector; same as ridge regression above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.03539807])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "# least absolute shrinkage and selection operator regression\n",
    "\n",
    "# adds regularization term equal to l1 norm, so is different\n",
    "# from ridge regression\n",
    "# it automatically selects features and outputs a sparse matrix\n",
    "# because it tends to eliminate weights of the least important features\n",
    "\n",
    "# note lasso cost is not differentiable at any theta = 0, but gd \n",
    "# still works fine if you use a subgradient vector instead when any\n",
    "# theta = 0\n",
    "\n",
    "# subgradient (g) = gradient(MSE) + alpha * (sign vector)\n",
    "\n",
    "# scikit, or could do sgd with penalty = l1\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha = .1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.03633024])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic Net is middle ground between two,\n",
    "# regularization term is mix of both l1 and l2,\n",
    "# and you control ratio r\n",
    "# if r = 0, it is ridge regression. if r = 1, it is lasso\n",
    "\n",
    "# when to use each of these? You should always have some\n",
    "# regularization, so it is preferable to avoid plain \n",
    "# linear regression\n",
    "# Ridge is a good default, but if you see that only a few\n",
    "# features are important, you should prefer Lasso or Elastic\n",
    "# because they reduce the useless features' weights down to zero\n",
    "\n",
    "# in general, elastic is preferred over Lasso because Lasso may behave erratically\n",
    "# when number of features is greater than number of training instances or when\n",
    "# several features are strongly correlated\n",
    "\n",
    "# scikit elastic net\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha = .1, l1_ratio = .5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "\n",
    "# you could also stop iterative learning algorithms when the\n",
    "# validation error reaches a minimum. The val error will be\n",
    "# at a minimum, then it will start to go up again, meaning\n",
    "# the model is beginning to overfit the training data\n",
    "\n",
    "# or can stop when validation error has been above min for some\n",
    "# time and roll back to previous model where val error was at min\n",
    "\n",
    "# implementation\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# prepare data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2)\n",
    "\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree = 90, include_bias = False)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter = 1, tol = -np.infty, warm_start = True, \n",
    "                       penalty = None, learning_rate = 'constant', eta0 = .0005)\n",
    "\n",
    "minimum_val_error = float('inf')\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train.ravel()) #continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)\n",
    "        \n",
    "# when warm start = True, it continues training from where\n",
    "# it left off rather than training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# commonly used to estimate probability that instance belongs\n",
    "# to a particular class. binary classifier would be like\n",
    "# greater than 50 predict 1, less than 50 predict 0\n",
    "\n",
    "# takes input features and computes weighted sum of them, \n",
    "# plus bias term (just like linear regression), and outputs the\n",
    "# logistic of this result\n",
    "\n",
    "# logistic is a sigmoid function that outputs a number between\n",
    "# 0 and 1\n",
    "\n",
    "# sigma(t) = 1 / (1 + exp(-t))\n",
    "\n",
    "# once model has estimated probability that x belongs to positive class,\n",
    "# can make its prediction y hat easily\n",
    "\n",
    "# training objective is to assign high probs to positive instances\n",
    "# and low probs to negative instances\n",
    "\n",
    "# cost function\n",
    "# c(theta) = -log(p hat) , if y = 1\n",
    "# -log(1 - p hat), if y = 0\n",
    "\n",
    "# this is the cost for one example, but can be rewritten over whole\n",
    "# training set as log loss\n",
    "\n",
    "# bad news is there is no known closed form solution for this\n",
    "# but the function is convex, so gradient descent (or other optimization algorithm)\n",
    "# is guaranteed to find global minimum\n",
    "\n",
    "# partial derivative of cost function with regard to jth model param theta_j is\n",
    "# 1/m * sum(sigmoid(theta^T*x^i) - y^i)x^i_j\n",
    "# computes prediction error and multiplies by jth feature value and averages all of them together\n",
    "# once you compute this vector, use it in batch gradient descent\n",
    "# this is for batch. for stochastic, just do one at a time. for mini-batch, use a minibatch at a time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will demonstrate logistic regression with iris dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we will try to determine simply from petal width\n",
    "# if the flower is iris virginica\n",
    "\n",
    "X = iris['data'][:, 3:] # petal width\n",
    "y = (iris['target'] == 2).astype(np.int) # 1 if iris virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now train model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5828e5ee20>]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], 'g-', label = \"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], 'b--', label = \"Not Iris virginica\")\n",
    "# clear decision boundary at 1.6. \n",
    "# x axis is petal length, y axis is probability\n",
    "# blue is not virginica, green is virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just like other linear models, logr can\n",
    "# be regularized with l1 or l2 norm. scikit\n",
    "# uses l2 for logr by default\n",
    "\n",
    "# Softmax Regression\n",
    "# this is logr with support for multiple classes\n",
    "# without having to train and combine multiple binary\n",
    "# classifiers\n",
    "# also called multinomial logistic regression\n",
    "\n",
    "# given instance x, softmax regression model first\n",
    "# computes a score s_k(x) for each class k, then estimates\n",
    "# the probability of each class by applying the softmax function (or normalized exponential)\n",
    "# score function is just like linear regression prediction\n",
    "\n",
    "# s_k(x) = x^T * theta^k\n",
    "# each class has own dedicated param vector theta^k\n",
    "# all these are typically stored in a parameter matrix\n",
    "\n",
    "# after computing the score for every class for the instance x\n",
    "# , estimate probability p hat _ k with softmax function\n",
    "\n",
    "# function computes exponential of every score, then normalizes them (divide by sum of all exponentials)\n",
    "# scores are generally called logits or log-odds\n",
    "\n",
    "# p hat_k = sigmoid(score(x)_k) = exp(score(x)) / sum j = 1 to k (score_j(x))\n",
    "# k is number of classes, s(x) is vector containing scores of each class for instance x\n",
    "# sigmoid function is estimated probability that instance x belongs to class k, given\n",
    "# the scores of each class for that instance\n",
    "\n",
    "# so classifier just predicts class with highest estimated probability\n",
    "# y hat = argmax_k(sigmoid(score(x))) = argmax_k(score_k(x)) = argmax_k((param vector k)^T*x)\n",
    "# argmax returns value of variable that maximizes a function, so in this case is k that\n",
    "# maximizes estimated probability of sigmoid function\n",
    "\n",
    "# softmax regression is multiclass, not multioutput. use is for mutually\n",
    "# exclusive classes like different types of plants, not to\n",
    "# recognize multiple people in a picture\n",
    "\n",
    "# training objective is to have a model that estimate a high probability\n",
    "# for the target class, and low probability for the other classes\n",
    "# minimizing the cost function (cross entropy) should lead to this objective\n",
    "# because it penalizes the model when it estimates low probability for a target class\n",
    "# cross entropy is frequently used to measure how well a set of estimated class probabilities\n",
    "# matches the target classes\n",
    "\n",
    "# cross entropy = -1/m sum 1 to m (sum 1 to k (y_k^(i) * log(p hat _k ^ (i))))\n",
    "# y_k^(i) is target probability that ith instance belongs to class k. in general,\n",
    "# is either 1 or 0, depending upon whether instance belongs to class or not\n",
    "\n",
    "# when there are 2 classes, this function is simply equal to logistic \n",
    "# regression's cost function\n",
    "\n",
    "# the gradient for cross entropy is given as follows\n",
    "# = 1/m sum 1 to m (p hat_k^(i) - y_k^(i))x^(i)\n",
    "\n",
    "# now we can compute gradient vector for every class then use gradient descent (or\n",
    "# other optimization algorithm) to find parameter matrix that minimizes the cost function\n",
    "\n",
    "# now, use softmax regression to classify iris flowers into all three classes\n",
    "\n",
    "# scikit learn uses one versus rest by default when you train it on more than\n",
    "# two classes, but can set multi_class hyperparameter to multinomial to switch\n",
    "# to softmax regression\n",
    "# must also specify a solver that supports softmax regression, such as lbfgs\n",
    "# it applies l2 regularizer by default, which you can control with hyperparameter C\n",
    "\n",
    "X = iris['data'][:, (2, 3)] # use petal length, petal width\n",
    "y = iris['target']\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', C = 10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5, 2]]) # 5 cm long, 2 cm wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5, 2]])\n",
    "# third option has highest probability, and that's what it gave us above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
