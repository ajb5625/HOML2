{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "# y hat = theta0 + theta1x1 + ... + thetanxn\n",
    "# vectorized form : y hat = parameter vector * feature vector\n",
    "\n",
    "# to find theta vector that minimizes the cost (MSE)\n",
    "# there is a closed form solution = The Normal Equation\n",
    "# theta hat = (XTX)^-1 XT y\n",
    "# theta hat is value that minimizes cost function\n",
    "# y is vector of target values\n",
    "# now we test this equation as follows\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "# generated linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.76656781],\n",
       "       [3.13971004]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, compute theta hat using normal equation\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best\n",
    "\n",
    "# the function we used to generate the data was\n",
    "# y = 4 + 3x + gaussian noise\n",
    "# so we would've hoped for theta0 to be 4 and theta1 to be 3, but close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.76656781],\n",
       "       [10.04598789]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now can make predictions using theta hat\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict\n",
    "\n",
    "# pretty neat, the underworkings of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets plot it\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0,2,0,15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.76656781]), array([[3.13971004]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do simply with scikit\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "# same as what we calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.76656781],\n",
       "       [10.04598789]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)\n",
    "# and same as prediction above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.76656781],\n",
       "       [3.13971004]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the linear regression class is based on scipy.linalg.lstsq\n",
    "# meaning least squares, can call directly\n",
    "\n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond = 1e-6)\n",
    "theta_best_svd\n",
    "\n",
    "# same as two above\n",
    "# this function actually computes\n",
    "# theta hat = X^+y, where X^+ is the pseudoinverse of X\n",
    "# (Moore-Penrose inverse). can use np.linalg.pinv() to compute \n",
    "# pseudoinverse directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.76656781],\n",
       "       [3.13971004]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)\n",
    "\n",
    "# the purpose of the pseudoinverse in linear algebra is to\n",
    "# compute the best fit (least squares, eg linear regression) \n",
    "# solution to a system of linear equations that lacks a solution\n",
    "\n",
    "# another use is to find the minimum euclidean norm solution\n",
    "# to a system of linear equations with multiple solutions\n",
    "\n",
    "# it is computed using standard matrix factorization technique\n",
    "# called Singular Value Decomposition (SVD) that decomposes\n",
    "# the training set X into matrix multiplication of three \n",
    "# matrices U, sigma, and V^T (np.linalg.svd())\n",
    "# the pseudoinverse is computed as X^+ = V sigma^+ U^T\n",
    "\n",
    "# to compute sigma^+, algo takes sigma and sets to zero all values\n",
    "# smaller than a tiny threshold. after, it replaces all nonzero values with \n",
    "# their inverse and transposes the resulting matrix\n",
    "\n",
    "# this computation is more efficient than the normal equation\n",
    "# also, the normal equation may not work if XTX is not invertible,\n",
    "# but the pseudoinverse is always defined\n",
    "\n",
    "# normal equation complexity is between O(n^2.4) and O(n^3)\n",
    "# while scikit svd is O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent is better suited method to train\n",
    "# linear regression model when there are a large\n",
    "# number of features or too many training instances to fit\n",
    "# in memory\n",
    "\n",
    "# move in direction of steepest descent by computing the gradient\n",
    "# at the current point. when gradient is 0, ou have found minimum\n",
    "\n",
    "# start parameter vector with random values and improve gradually taking one\n",
    "# step at a time, attempting to decrese the cost function until\n",
    "# algorithm converges at mnimum\n",
    "\n",
    "# important hyperparameter is learning rate. if too small, it will take \n",
    "# too many steps. if too large, it could oscillate and could diverge\n",
    "\n",
    "# if cost function is not bowl shape, could converge to local min\n",
    "# rather than global optimal min\n",
    "\n",
    "# mse is convex function, meaning pick any two points across\n",
    "# the curve and line connecting them never crosses the curve\n",
    "# meaning no local minima, just global minimum\n",
    "# it also is continuous (can take a derivative) and never\n",
    "# changes slope abruptly\n",
    "\n",
    "# when using gradient descent, make sure all features have similar scale\n",
    "# ie use standard scaler\n",
    "\n",
    "# training is more difficult for this in higher dimensions; thankfully for linear regression\n",
    "# the solution is simply at the bottom of the bowl (cause convex function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.76656781],\n",
       "       [3.13971004]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch gradient descent\n",
    "# takes partial derivative of cost function with respect to each\n",
    "\n",
    "# partial derivative of cost function = \n",
    "# 2/m * sum(parameter .dot features - labels)*feature\n",
    "\n",
    "# parameter thetaj\n",
    "# to determine, take partials and move in opposite direction of slope\n",
    "# times the learning rate to get step size\n",
    "# theta step = theta - rate * gradient(mse)\n",
    "\n",
    "eta = .1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta\n",
    "# and is exactly what every other method found\n",
    "# to find a good learning rate, can use grid search from chapter 2\n",
    "# to find iterations, set to large number and break when gradient\n",
    "# vector becomes tiny, smaller than tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
