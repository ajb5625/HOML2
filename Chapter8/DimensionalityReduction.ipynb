{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 2d manifold is a shape that can be bent and twisted in a higher dimensional space\n",
    "# like a 3d swiss roll. cannot just project the data onto a 2d plane, as it would be squished\n",
    "\n",
    "# dimensionality reduction depends on the dataset. it may or may not lead to a better or simpler solution\n",
    "# but it will always reduce training time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "# identify the hyperplane closest to the data and project onto it\n",
    "\n",
    "# we need to pick the hyperplane that preserves variance\n",
    "# pick the axis that minimizes the mean squared distance between the original\n",
    "# dataset and its projection onto the new axis\n",
    "\n",
    "# singular value decomposition can get the principal components\n",
    "\n",
    "# once you have pcs, reduce dims by projecting it onto hyperplane defined by first d pcs. this projects\n",
    "# down to d dimensions\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the amount of variance which lies in each pc with explained variance ration\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learn will automatically determine if randomized PCA is better\n",
    "# there are incremental pca algos that don't require you to feed the whole dataset\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)\n",
    "\n",
    "# or you could use numpy memmap to only read in parts of array you need\n",
    "\n",
    "X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m,n))\n",
    "\n",
    "batch_size = m // n\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is also kernel pca to preserve clusters of instances after projection or unroll datasets\n",
    "# performs complex nonlinear projections\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# could figure out best kernel using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lle is local linear embedding\n",
    "# manifold learning technique which looks at relationships between close data items and preserves local\n",
    "# relationships but not global ones\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
