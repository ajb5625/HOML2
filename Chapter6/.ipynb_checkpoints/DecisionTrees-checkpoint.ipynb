{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dt classifier on iris dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can visualize a dt using graphviz\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf,\n",
    "               out_file = 'iris_tree.dot',\n",
    "               feature_names = iris.feature_names[2:],\n",
    "               class_names = iris.target_names,\n",
    "               rounded = True,\n",
    "               filled = True\n",
    "               )\n",
    "\n",
    "# note that gini measures impurity\n",
    "# if all 100 classes apply on leaf node, gini = 0, if some are wrong it will not equal 0\n",
    "\n",
    "# sklearn uses the cart algorithm which will only produce binary trees. id3 can produce\n",
    "# trees with any number of children\n",
    "\n",
    "# white box vs black box\n",
    "\n",
    "# dt is white box model, as you can easily tell why it made the classification it did.\n",
    "# random forests and neural networks are black box, because you can understand the computation\n",
    "# but not really how the classification was made\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also output class probabilities by going to leaf node with \n",
    "# certain properties and dividing samples by total\n",
    "\n",
    "tree_clf.predict_proba([[5, 1.5]]) # 5 cm long, 1.5 cm wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])\n",
    "# just tells which class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit uses cart (classification and regression tree) algorithm\n",
    "# splits dataset into two subsets using a feature k and threshold tk\n",
    "# k and tk are chosen by pair that produces purest subset weighted by size\n",
    "# does this recursively to build tree until stopping conditions satisfied\n",
    "# greedy algorithm\n",
    "\n",
    "# prediction is O(log(m)), while training is O(nmlog(m)) because it looks at all features\n",
    "# on all samples at each node. for small datasets, scikit can speed up training by presorting\n",
    "# the data (presort = True) but this will slow down considerably for large sets\n",
    "\n",
    "# instead of gini impurity, we can measure with entropy by setting criterion hyperparameter to entropy.\n",
    "# reduction of entropy is often called information gain. entropy close to 0 means everything is well ordered,\n",
    "# as entropy originated from uncertainty of molecules in thermodynamics.\n",
    "# a set's entropy is zero when it contains instances of only one class\n",
    "\n",
    "# entropy tends to produce more balanced trees. gini has faster computation and isolates most frequent class in\n",
    "# its own branch of the tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization\n",
    "\n",
    "# dts will overfit very easy, as it is a non-parametric model meaning the number of params is not determined\n",
    "# prior to training. parametric is the opposite, like a linear model has a predetermined number of params.\n",
    "# because parametric has limited degrees of freedom, it reduces the risk of overfitting but increases\n",
    "# risk of underfitting\n",
    "\n",
    "# one way is max_depth hyperparam. min_samples_split is minimum number of samples a node must\n",
    "# have before it can split. min_samples_leaf is minimum number of samples a leaf node must have.\n",
    "# min_weight_fraction_leaf is same as min samples leaf but expressed as a fraction of total number of \n",
    "# weighted instances. max_leaf_nodes is max number of leaf nodes, max_features is max number of features\n",
    "# evalutated for splitting at each node\n",
    "# increasing min_* or reducing max_* will regularize the model\n",
    "\n",
    "# other algorithms build the tree and then prune it. it will get rid of unnecessary leaf nodes by trying\n",
    "# to see if the leaf is statistically significant using the chi squared test. the null hypothesis is that\n",
    "# the improvement is purely chance. if p-value is less than threshold, typically 5% (hyperparameter), then node is \n",
    "# dropped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "# will train on noisy quadratic dataset with max depth of 2\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg.fit(X, y)\n",
    "\n",
    "# prediction value for each region is average target value of instances in that region with mean squared error\n",
    "\n",
    "# cart algorithm works similarly, instead of minimizing impurity it now tries to minimize mean squared error\n",
    "# this still overfits very badly, so remember to set hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instability\n",
    "\n",
    "# one issue is the nature of the decision boundary in dts to be orthogonal to an axis\n",
    "# simple linearly separable dataset down the middle is rotated 45 degrees; now decision boundary\n",
    "# looks like a staircase instead of a line. PCA can reduce this kind of issue.\n",
    "\n",
    "# dts are very sensitive to small variations in the training data. you can get very different\n",
    "# models based on the same training data because scikit dt algorithm is stochastic (random)\n",
    "# removing the widest petaled iris versicolor created a whole different model\n",
    "# (unless you set the random_state hyperparameter)\n",
    "\n",
    "# Random Forests can limit this instability by averaging predictions of many trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
