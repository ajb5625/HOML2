{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# SVMs can be used for linear or nonlinear classification\n",
    "# regression, and outlier detection\n",
    "\n",
    "# Linear SVM\n",
    "# creates a decision boundary with largest possible margin,\n",
    "# margin is distance from line to nearest point of class\n",
    "# having a margin allows for new instances to be classified better\n",
    "# than if the separating hyperplane had a smaller margin\n",
    "# svms are sensitive to feature scaling, use some scaler on data\n",
    "# finds the widest possible street(margin), the linstorylines going through nearest\n",
    "# points are called support vectors\n",
    "# adding more instances outside of the street in training will not affect \n",
    "# decision boundary at all, only in the street (between support vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soft Margin Classification\n",
    "\n",
    "# hard margin classification is when all instances are off of \n",
    "# the street and are on the right side\n",
    "# issues: will only work if data is linearly separable\n",
    "# it is sensitive to outliers\n",
    "\n",
    "# we can avoid these issues with a more flexible model\n",
    "# find a good balance between keeping the margin wide and limiting\n",
    "# margin violations. this is soft margin classification\n",
    "\n",
    "# svm in scikit, we can specify hyperparams\n",
    "# C value is how much we want to avoid misclassifying each example\n",
    "# higher could potentially overfit\n",
    "\n",
    "# we want fewer margin violations, but keep in mind bias/variance\n",
    "# tradeoff in order to generalize better\n",
    "\n",
    "# load iris, scale features, train linear SVM\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)] # petal length and width\n",
    "y = (iris['target'] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C = 1, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])\n",
    "# does not output a probability like logistic regression,\n",
    "# but simply classifies\n",
    "\n",
    "# instead of LinearSVC, we could use SVC class with linear kernel\n",
    "# or SGDClassifier with hinge loss and alpha = 1/(m*C)\n",
    "# it will not converge as fast but will be useful to handle online\n",
    "# classification tasks or huge datasets that don't fit in memory\n",
    "\n",
    "# LinearSVC regularizes bias term, so center training set by first subtracting\n",
    "# the mean. StandardScaler does this. hinge loss is not default, so remember to set it\n",
    "# for performance, set dual to false unless there are more features than training\n",
    "# instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nonlinear SVM Classification\n",
    "\n",
    "# you could add more features like we did previously, (x, x^2)\n",
    "\n",
    "# we can do this in scikit with pipeline of PolynomialFeatures, \n",
    "# StandardScaler, and LinearSVC\n",
    "\n",
    "# test on moons dataset, data points are shaped as two\n",
    "# interleaving half-circles\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples = 100, noise = .15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree = 3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C = 10, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "\n",
    "# adding polynomial features is simple and can work greate with ml algorithms\n",
    "# low degree will not likely help with complex data, and high degree creates\n",
    "# too many features making the model too slow\n",
    "\n",
    "# there is a kernel trick for SVM that appears as if you added many polynomial\n",
    "# features, without having to add them. therefore there is no combinatorial\n",
    "# explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel = 'poly', degree = 3, coef0 = 1, C = 5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "# this trains a SVM using third degree polynomial kernel. higher \n",
    "# degree will overfit, lower degree will underfit. coef0 controls how\n",
    "# much the model is influenced by high degree polynomials versus\n",
    "# low degree polynomials\n",
    "\n",
    "# can find best hyperparameters using grid search (chapter 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also transform nonlinear data into linear data\n",
    "# by using a Similarity function like Gaussian\n",
    "# Radial Basis Function\n",
    "# plug into this function to compute new features\n",
    "\n",
    "# phi(x, l) = exp(-y||x-l||^2)\n",
    "# after transforming data, it will be linearly separable\n",
    "# this can cause high dimensionality though, because\n",
    "# it will create a landmark for every data instance\n",
    "\n",
    "# a landmark is an object to compare to the data instance\n",
    "# and tell how similar it is\n",
    "\n",
    "# similar to polynomial features method, this can be used\n",
    "# with any ml algorithm but can be computationally expensive\n",
    "\n",
    "# the kernel trick is at play here, making it possible to \n",
    "# have a similar result as if you had added many similarity features\n",
    "\n",
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel = 'rbf', gamma = 5, C = .001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "# increasing gamma makes bell curve narrower\n",
    "# this in turn makes the decision boundary change\n",
    "# high gamma will be more complex and irregular,\n",
    "# low gamma will be more smooth and easy\n",
    "\n",
    "# gamma is hyperparam, if overfitting you should reduce it,\n",
    "# if underfitting, you should increase it\n",
    "\n",
    "# C param tells how strict you are on misclassification,\n",
    "# the higher it is, the closer the decision boundary will be\n",
    "# to encompassing all correct points on training data\n",
    "\n",
    "# other kernels exist but are far less common\n",
    "# there is string kernel for classifying documents or DNA\n",
    "# sequences, based on Levenshtein Distance (single character edit(insert, delete, swaps) required\n",
    "# to change one word into another)\n",
    "\n",
    "# to choose kernel, always try LinearSVC before SVC(kernel = 'linear') cause LinearSVC is faster\n",
    "# if training set not too large, try Gaussian rbf. If extra computing power, use cross validation\n",
    "# and grid search to find best combo\n",
    "\n",
    "# LinearSVC is O(m * n) and algorithm will take longer according to tolerance parameter\n",
    "# in most classifications, default tolerance is fine. does not support kernel trick\n",
    "\n",
    "# SVC supports kernel trick, but it is  between O(m^2 * n) O(m^3 * n). oof\n",
    "# m is training instances, n is number of features. algorithm is good for\n",
    "# complex small or medium training data. scales well with sparse features\n",
    "# (each instance has few nonzero features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(degree=2, kernel='poly')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Regression\n",
    "\n",
    "# svm can also be used for linear and nonlinear regression\n",
    "# objective is to fit as many instances as possible on the\n",
    "# street (in margin) while limiting margin violations\n",
    "# width of margin is hyperparameter epsilon. adding more\n",
    "# training examples within the margin does not affect\n",
    "# the model's prediction, thus the model is epsilon insensitive\n",
    "\n",
    "# note you should center and scale data first\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon = 1.5)\n",
    "svm_reg.fit(X, y)\n",
    "\n",
    "# for nonlinear regression, use kernelized svm model\n",
    "\n",
    "# regularization again dependent on C, high C value is little,\n",
    "# low C value is a lot\n",
    "# low c value is large margin, high c value is small margin.\n",
    "# therefore regularizes by telling how wrong you can be\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel = 'poly', degree = 2, epsilon = .1)\n",
    "svm_poly_reg.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does it work?\n",
    "\n",
    "# the linear svm predicts using a sign function of w^Tx + b\n",
    "# if it is less than 0, then negative class. if greater, positive class\n",
    "\n",
    "# for the data with petal width and length, it forms two planes\n",
    "# and the decision boundary is the intersection of those two planes\n",
    "# the dashed lines are where the decision function equals positive or negative classes\n",
    "# forming a margin. the goal of training is to find values of w and b that\n",
    "# make the margin as wide as possible while avoiding or limiting margin violations\n",
    "\n",
    "# the overall goal is to minimize the weight vector's magnitude. this is because\n",
    "# the slope of the decision boundary is equal to the magnitude of the weight vector.\n",
    "# therefore, smaller weight vector means wider margin. also subject to the constraint of\n",
    "# wanting greater than 1 for positive instances, less than -1 for negative training instances\n",
    "\n",
    "# therefore\n",
    "# minimize 1/2 w^Tw\n",
    "# subject to t^(i)(w^Tx^(i) + b) >= 1 for i = 1, 2, ..., m\n",
    "# (we add the 1/2 because it has an easy derivative, just w! while magnitude w is not differentiable\n",
    "# at w = 0. optimization algorithms work better on differentiable functions)\n",
    "\n",
    "# for soft margin, we introduce slack variable zeta^(i) which measures how much the ith instance\n",
    "# is allowed to violate the margin\n",
    "\n",
    "# so now the objectives conflict; make the slack variable as small as possible and make 1/2w^Tw as small\n",
    "# as possible to increase the width of the margin. This is where the C value comes in, C is the tradeoff\n",
    "# ratio of the two objectives. so the constrained optimization problem for soft svm is as follows\n",
    "\n",
    "# minimize 1/2w^Tw + C sum(all slack variables)\n",
    "# subject to t^(i)(w^Tx^(i) + b) >= 1 - slack at ith instance and slack at i >= 0 for i = 1, 2, ... m\n",
    "\n",
    "# hard and soft margin problems are convex quadratic optimization problems with linear constraints.\n",
    "# they are known as Quadratic Programming (QP) problems. there are solutions to these problems\n",
    "# outside the scope of this material\n",
    "\n",
    "# general form of qp\n",
    "# minimize x subject to y where z, usually having to do with matrices\n",
    "\n",
    "# one way to train hard margin linear svm is to use off shelf qp solver and pass it the right parameters\n",
    "\n",
    "# The Dual Problem\n",
    "\n",
    "# given a constrained optimization problem known as the primal problem, it is possible to express it as a different\n",
    "# but closely related problem called the dual problem. The solution of the dual problem typically gives\n",
    "# a lower bound to the solution of the primal problem, but under some conditions can give the same solution\n",
    "# as the primal problem. in SVM, the dual and primal problems have the same solution. also, the dual problem\n",
    "# makes the kernel trick possible, while the primal problem does not. \n",
    "\n",
    "# Kernelized SVMs (kernel trick)\n",
    "\n",
    "# when you transform dimensions, the dot product of the transformed vectors is just equal to the square of the \n",
    "# dot product of the original vectors. \n",
    "# phi(a)^T phi(b) = (a^Tb)^2\n",
    "# so instead of transforming the training data, just repalce the dot product by its square\n",
    "# a kernel in machine learning is a function capable of computing the dot product of vectors\n",
    "# based only on the original vectors a and b without having to compute or know about the transformation\n",
    "# phi. common kernels include linear, polynomial, gaussian rbf, and sigmoid\n",
    "\n",
    "# follows Mercer's Theorem. if a function represents a few mathematicla conditions called Mercer's Conditions\n",
    "# (k is continuous and symmetric in its arguments, etc) then there is a function phi that maps a and b into another\n",
    "# space. so you can use the function as a kernel because you know phi exists, but you don't even have to know what\n",
    "# it is. some frequently used kernels (like sigmoid) don't respect all of Mercer's conditions, but they \n",
    "# work well in practice anyways\n",
    "\n",
    "# Online SVMs\n",
    "\n",
    "# one method for linear svc is to use gradient descent, but this converges slower than using qp. \n",
    "\n",
    "# Hinge Loss\n",
    "# max (0, 1 - t). it is 0 when t >= 1. its derivative (slope) is equal to -1 if t < 1 and 0 if t > 1.\n",
    "# it isn't differentiable at t = 1, but you can still use gradient descent by using any subderivative at t = 1.\n",
    "\n",
    "# you can also implement online kernelizd svms, they are implemented in matlab and C++. for large scale nonlinear\n",
    "# problems, you should probably use neural networks instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
