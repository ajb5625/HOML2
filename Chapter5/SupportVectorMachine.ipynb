{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# SVMs can be used for linear or nonlinear classification\n",
    "# regression, and outlier detection\n",
    "\n",
    "# Linear SVM\n",
    "# creates a decision boundary with largest possible margin,\n",
    "# margin is distance from line to nearest point of class\n",
    "# having a margin allows for new instances to be classified better\n",
    "# than if the separating hyperplane had a smaller margin\n",
    "# svms are sensitive to feature scaling, use some scaler on data\n",
    "# finds the widest possible street(margin), the lines going through nearest\n",
    "# points are called support vectors\n",
    "# adding more instances outside of the street in training will not affect \n",
    "# decision boundary at all, only in the street (between support vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soft Margin Classification\n",
    "\n",
    "# hard margin classification is when all instances are off of \n",
    "# the street and are on the right side\n",
    "# issues: will only work if data is linearly separable\n",
    "# it is sensitive to outliers\n",
    "\n",
    "# we can avoid these issues with a more flexible model\n",
    "# find a good balance between keeping the margin wide and limiting\n",
    "# margin violations. this is soft margin classification\n",
    "\n",
    "# svm in scikit, we can specify hyperparams\n",
    "# C value is how much we want to avoid misclassifying each example\n",
    "# higher could potentially overfit\n",
    "\n",
    "# we want fewer margin violations, but keep in mind bias/variance\n",
    "# tradeoff in order to generalize better\n",
    "\n",
    "# load iris, scale features, train linear SVM\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)] # petal length and width\n",
    "y = (iris['target'] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C = 1, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])\n",
    "# does not output a probability like logistic regression,\n",
    "# but simply classifies\n",
    "\n",
    "# instead of LinearSVC, we could use SVC class with linear kernel\n",
    "# or SGDClassifier with hinge loss and alpha = 1/(m*C)\n",
    "# it will not converge as fast but will be useful to handle online\n",
    "# classification tasks or huge datasets that don't fit in memory\n",
    "\n",
    "# LinearSVC regularizes bias term, so center training set by first subtracting\n",
    "# the mean. StandardScaler does this. hinge loss is not default, so remember to set it\n",
    "# for performance, set dual to false unless there are more features than training\n",
    "# instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nonlinear SVM Classification\n",
    "\n",
    "# you could add more features like we did previously, (x, x^2)\n",
    "\n",
    "# we can do this in scikit with pipeline of PolynomialFeatures, \n",
    "# StandardScaler, and LinearSVC\n",
    "\n",
    "# test on moons dataset, data points are shaped as two\n",
    "# interleaving half-circles\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples = 100, noise = .15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree = 3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C = 10, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "\n",
    "# adding polynomial features is simple and can work greate with ml algorithms\n",
    "# low degree will not likely help with complex data, and high degree creates\n",
    "# too many features making the model too slow\n",
    "\n",
    "# there is a kernel trick for SVM that appears as if you added many polynomial\n",
    "# features, without having to add them. therefore there is no combinatorial\n",
    "# explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel = 'poly', degree = 3, coef0 = 1, C = 5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "# this trains a SVM using third degree polynomial kernel. higher \n",
    "# degree will overfit, lower degree will underfit. coef0 controls how\n",
    "# much the model is influenced by high degree polynomials versus\n",
    "# low degree polynomials\n",
    "\n",
    "# can find best hyperparameters using grid search (chapter 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
